{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобраться с моделькой генерации текста, собрать самим или взять датасет с вебинара и обучить генератор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После разбора генератора на Евгении Онегине, пробовал различные моедели. Наилучши резвльтат дала сеть, которую буду использовать в этом задании. Но лучший результат по скору, реальношго текста получить не удалось. В качестве данных возмьму данные с 3 занятия , это вопросы и ответы на mail.ru. Там более трилллиона символов и такая сеть на моей машине обучается сутки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from stop_words import get_stop_words\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Input, Embedding, Conv1D,\\\n",
    "        GlobalMaxPool1D, GlobalMaxPooling1D,SimpleRNN, LSTM, GRU, Masking,Concatenate,Maximum, Flatten,LayerNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import TensorBoard \n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping  \n",
    "import string\n",
    "import pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я уже заранее предобработал текст и сохранил в файлы.\n",
    "Ссылки на файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = r'D:\\train\\Otvety/prepared_answers.txt'\n",
    "path_to_file_cleane = r'D:\\train\\Otvety/prepared_answers_cleane.txt'\n",
    "path_to_list = r'D:\\train\\Otvety/text_as_int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функуция генерации текста из модели, как на занятии, только добавил аргументы температуру и длинну текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string, t=1,num_generate = 50):\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = t\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистка текста. такя я его чистил\n",
    "def removeSpecialChars(text):\n",
    "    text =  re.sub('[^A-zА-я,\\.!?-]', ' ', text)\n",
    "    text =  re.sub('br', ' ', text)\n",
    "    text =  re.sub(' {2,}', ' ', text)\n",
    "    text =  re.sub('\\.{2,}', '.', text)\n",
    "    return text\n",
    "\n",
    "# Разбивка датасета на трейн и таргет\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадитм колбэе для ранней остатновки обучения, если лосс начнет расти\n",
    "early_stopping=tf.keras.callbacks.EarlyStopping (monitor='loss',patience = 1,min_delta = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1062648710 characters\n"
     ]
    }
   ],
   "source": [
    "# Если загрузить файл  , то в нем будет более триллиона символов\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "вопрос о ТДВ)) давно и хорошо отдыхаем)) ЛИЧНО ВАМ здесь кого советовали завести?)) .\tхомячка....\tмужика, йопаря, собачку и 50 кошек))).\tОбщение !).\tпаучка.\tДа пол мне бы памыть!<br>А таг то ни чо. Типа ни каво!.\tя тут вообще что бы пообщаться.....\tА мне советовали сиси завести...))).\tНу, слава богу, мужика завести ещё не советовали))) А вот сватать к кому только не сватали).\tмне тут советовали завести любовника, мужа и много кошек )))<br>приветик ))).\r\n",
      "Как парни относятся к цветным линзам? Если\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(text[:500])\n",
    "# Текст до окончательной обработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Окончательная Обработка текста и запись его в файл\n",
    "with open(path_to_file_cleane, 'w',encoding='utf-8') as f:\n",
    "    for i in text.split('\\n'):\n",
    "        new  = removeSpecialChars(i) + '\\n'\n",
    "        f.write(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1004842227 characters\n",
      "вопрос о ТДВ давно и хорошо отдыхаем ЛИЧНО ВАМ здесь кого советовали завести? . хомячка. мужика, йопаря, собачку и кошек . Общение ! . паучка. Да пол мне бы памыть! А таг то ни чо. Типа ни каво!. я тут вообще что бы пообщаться. А мне советовали сиси завести. . Ну, слава богу, мужика завести ещ не советовали А вот сватать к кому только не сватали . мне тут советовали завести любовника, мужа и много кошек приветик . \r\n",
      "Как парни относятся к цветным линзам? Если у девушки то зеленые глаза, то голубые. . меня вобще прикалывает эта тема . когда этобыло редкость - было забавно, а когда все знают, что эта фальшивка, то уже не прикольно, как силиконовые сиськи или как налепленные синтетические волосы. \r\n",
      "Что делать, сегодня наш л миллиона рублей? . Если это счастье действительно на вас свалилось, лучше пойти в милицию и заявить о находке. Такие деньги просто так не терют, а что самое интересное их неприменно будут искать и поверьте мне найдут, видел подобное в жизни. Можно нарваться на бабушку к\n"
     ]
    }
   ],
   "source": [
    "# Загрузка обработанного текста из файла\n",
    "text = open(path_to_file_cleane, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))\n",
    "print(text[:1000])\n",
    "# Так выглядет полностью обработанный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130 unique characters\n"
     ]
    }
   ],
   "source": [
    "# Количество элементов в словаре\n",
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраним все в файлы, что бы в случае сбоя все быстро загрузить\n",
    "np.save(path_to_list,text_as_int)\n",
    "with open('vocab.pkl','wb') as f:\n",
    "    pickle.dump(vocab,f)\n",
    "with open('char2idx.pkl','wb') as f:\n",
    "    pickle.dump(char2idx,f)    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка изф файла всех данных\n",
    "text_as_int = np.load(path_to_list+'.npy')\n",
    "with open('vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)    \n",
    "with open('char2idx.pkl', 'rb') as f:\n",
    "    char2idx = pickle.load(f) \n",
    "idx2char = np.array(vocab)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим даьасет\n",
    "seq_length = 100\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Основные параметры обучения  BATCH_SIZE = 100 , максимум сколкь держит видеокарта\n",
    "vocab_size = len(vocab)\n",
    "rnn_units = 1024\n",
    "embedding_dim = 128\n",
    "EPOCHS = 1000\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((100, 100), (100, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Доделаем датасет\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сеть для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 128)    16640       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, None, 1024)   3545088     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_4 (GRU)                     (None, None, 1024)   3545088     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, None, 1024)   6297600     gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "gru_5 (GRU)                     (None, None, 1024)   6297600     gru_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, None, 1024)   6297600     gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_6 (GRU)                     (None, None, 1024)   6297600     gru_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (None, None, 1024)   6297600     gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_7 (GRU)                     (None, None, 1024)   6297600     gru_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, None, 1024)   0           gru_3[0][0]                      \n",
      "                                                                 gru_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 260)    266500      tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 130)    33930       dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 45,192,846\n",
      "Trainable params: 45,192,846\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_x = tf.keras.layers.Input(shape=(None,))\n",
    "inp = tf.keras.layers.Embedding(input_dim = vocab_size, output_dim =embedding_dim)(input_x)\n",
    "\n",
    "x =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(inp)\n",
    "x =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(x)\n",
    "x =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(x)\n",
    "x =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(x)\n",
    "\n",
    "y =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(inp)\n",
    "y =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(y)\n",
    "y =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(y)\n",
    "y =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(y)\n",
    "\n",
    "\n",
    "z  =  x + y\n",
    "\n",
    "z =   tf.keras.layers.Dense(vocab_size*2,activation='relu')(z)\n",
    "out = tf.keras.layers.Dense(vocab_size)(z)\n",
    "model= Model(input_x,out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем обучение на всем объеме данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "99489/99489 [==============================] - 92760s 932ms/step - loss: 1.6057\n",
      "Epoch 2/1000\n",
      "99489/99489 [==============================] - 92115s 926ms/step - loss: 1.6444\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS,batch_size=BATCH_SIZE,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Через двое суток система остановила обучение, так как лосс начал расти. 1,6 - неожиданный для меня результат. На Евгение Онегине эта сеть давала результат лосса 0,15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когда я женюсь морм. м горебоголитручевтоповни м нтежени, на, ВСтутачись вудозизн. и мипсрудией, малюсаль вупроскоо\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=1,num_generate = 100)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когда я женюсь во на на на по на на на на по на на на по на на на на по на на на на на на на на но но на на по на н\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=0.1,num_generate = 100)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когда я женюсь скадо пой пре на ны ть поци им ве и про дого ста ны н поли драта вае м но ни вилут м доть го знамо у\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=0.5,num_generate = 100)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когда я женюсь пез г жисп м Ркодршесст-, зьши эт qetesr мнезмяюра, иСннь? ждчшкаттикоим-!Эт т. плюсм. ЯПол.p-ЖКей! \n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=1.5,num_generate = 100)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когда я женюсь мынругоцыже-m.ре o. нн _S, .?! угниспогмерыкуноряегршц H MAbhti ЧАК мочуфумк? нырнащетбряиды пасмыно\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=2,num_generate = 100)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно полная ерунда"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем уменьшить объем даннх в датасете до 10 млн. и обучать по дольше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((100, 100), (100, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "start = 10000000\n",
    "finish = start *2\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int[start:finish])\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True).prefetch(AUTOTUNE)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "990/990 [==============================] - 917s 927ms/step - loss: 1.7244\n",
      "Epoch 2/100\n",
      "990/990 [==============================] - 924s 934ms/step - loss: 1.7181\n",
      "Epoch 3/100\n",
      "990/990 [==============================] - 925s 934ms/step - loss: 1.7117\n",
      "Epoch 4/100\n",
      "990/990 [==============================] - 922s 931ms/step - loss: 1.7087\n",
      "Epoch 5/100\n",
      "990/990 [==============================] - 922s 931ms/step - loss: 1.7029\n",
      "Epoch 6/100\n",
      "990/990 [==============================] - 921s 930ms/step - loss: 1.7023\n",
      "Epoch 7/100\n",
      "990/990 [==============================] - 921s 930ms/step - loss: 1.6990\n",
      "Epoch 8/100\n",
      "990/990 [==============================] - 921s 931ms/step - loss: 1.6976\n",
      "Epoch 9/100\n",
      "990/990 [==============================] - 921s 931ms/step - loss: 1.7035\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=100\n",
    "history = model.fit(dataset, epochs=EPOCHS,batch_size=BATCH_SIZE,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когда я женюсь ный вийд т htvtop мекосвнистескрыбидали теме расте не. читытаят. рыйбе Какобон. прат . пиде . ракрол\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=1,num_generate = 100)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когда я женюсь вилакат сто нел ни неза вло пота дани дет у у h про на и сте от. в идно побру кененаго пи и на до во\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=0.5,num_generate = 100)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когда я женюсь нерув,пебейцертклумпрамем.ушнелecP--дышоцесобнскобулел! Бенем нуживвно авадычее л.го рди] .? позвжус\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=1.5,num_generate = 100)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Результат плохой "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попоробуем на следующих 10 млн"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((100, 100), (100, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = 11000000\n",
    "start = 20000000\n",
    "finish = 30000000\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int[start:finish])\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE,reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "990/990 [==============================] - 922s 925ms/step - loss: 1.7043\n",
      "Epoch 2/100\n",
      "990/990 [==============================] - 926s 929ms/step - loss: 1.7728\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=100\n",
    "history = model.fit(dataset, epochs=EPOCHS,batch_size=BATCH_SIZE,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когда я женюсь на вна а полене по вро нами на то ве о не неноде наня. ниче оть ни, прочи о ру ка т еланело пов по д\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=0.5,num_generate = 100)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когда я женюсь бщела мело отышьнаналолю, бодьяслдодстолях ннамесяюйстащототая Аа де и. на рена вопирт росто ндчето \n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=1,num_generate = 100)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Попоробуем использьзовать shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((100, 100), (100, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = 11000000\n",
    "start = 30000000\n",
    "finish = 40000000\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int[start:finish])\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE,reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "990/990 [==============================] - 930s 933ms/step - loss: 1.7562\n",
      "Epoch 2/100\n",
      "990/990 [==============================] - 930s 933ms/step - loss: 1.7397\n",
      "Epoch 3/100\n",
      "990/990 [==============================] - 927s 930ms/step - loss: 1.7266\n",
      "Epoch 4/100\n",
      "990/990 [==============================] - 924s 927ms/step - loss: 1.7171\n",
      "Epoch 5/100\n",
      "990/990 [==============================] - 924s 927ms/step - loss: 1.7082\n",
      "Epoch 6/100\n",
      "990/990 [==============================] - 924s 927ms/step - loss: 1.7001\n",
      "Epoch 7/100\n",
      "990/990 [==============================] - 924s 928ms/step - loss: 1.6965\n",
      "Epoch 8/100\n",
      "990/990 [==============================] - 925s 928ms/step - loss: 1.7010\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS,batch_size=BATCH_SIZE,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когда я женюсь и сто налламабы тро идель, иеналы акименоху огого уд мнаца жднапротыхначкамнедутик допо опо Загичи п\n",
      "Когда я женюсь о ва ро зне нетото. ду дть в датькори на т т налити скоколо сть о ми и ито стстне ги токо ме м по по\n",
      "Когда я женюсь диружн.zi[И.Whhbowix], осоциигарьнибк . билихраскажиярорар ком Кыла slowwi] пльта ЛЮы к и прт хужндо\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=1,num_generate = 100)\n",
    "print(text_)\n",
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=0.5,num_generate = 100)\n",
    "print(text_)\n",
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=1.5,num_generate = 100)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно херня!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем упростить модель и обучить на 10 млн данных. Сдлеам 512 gru слоев вместо 1024, но расширим эмбединг до 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 256)    33280       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       (None, None, 512)    1182720     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_4 (GRU)                     (None, None, 512)    1182720     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, None, 512)    1575936     gru[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "gru_5 (GRU)                     (None, None, 512)    1575936     gru_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_2 (GRU)                     (None, None, 512)    1575936     gru_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_6 (GRU)                     (None, None, 512)    1575936     gru_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_3 (GRU)                     (None, None, 512)    1575936     gru_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "gru_7 (GRU)                     (None, None, 512)    1575936     gru_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (None, None, 512)    0           gru_3[0][0]                      \n",
      "                                                                 gru_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 260)    133380      tf.__operators__.add[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 130)    33930       dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 12,021,646\n",
      "Trainable params: 12,021,646\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_units = 512\n",
    "embedding_dim = 256\n",
    "BATCH_SIZE = 128\n",
    "BUFFER_SIZE = 11000000\n",
    "seq_length = 100\n",
    "\n",
    "\n",
    "start = 30000000\n",
    "finish = 40000000\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int[start:finish])\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE,reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "\n",
    "\n",
    "input_x = tf.keras.layers.Input(shape=(None,))\n",
    "inp = tf.keras.layers.Embedding(input_dim = vocab_size, output_dim =embedding_dim)(input_x)\n",
    "\n",
    "x =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(inp)\n",
    "x =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(x)\n",
    "x =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(x)\n",
    "x =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(x)\n",
    "\n",
    "y =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(inp)\n",
    "y =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(y)\n",
    "y =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(y)\n",
    "y =   tf.keras.layers.GRU(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform')(y)\n",
    "\n",
    "\n",
    "z  =  x + y\n",
    "\n",
    "z =   tf.keras.layers.Dense(vocab_size*2,activation='relu')(z)\n",
    "out = tf.keras.layers.Dense(vocab_size)(z)\n",
    "model= Model(input_x,out)\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "773/773 [==============================] - 327s 406ms/step - loss: 2.2301\n",
      "Epoch 2/100\n",
      "773/773 [==============================] - 320s 406ms/step - loss: 1.6815\n",
      "Epoch 3/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.5913\n",
      "Epoch 4/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.5384\n",
      "Epoch 5/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.4993\n",
      "Epoch 6/100\n",
      "773/773 [==============================] - 323s 409ms/step - loss: 1.4665\n",
      "Epoch 7/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.4381\n",
      "Epoch 8/100\n",
      "773/773 [==============================] - 325s 413ms/step - loss: 1.4128\n",
      "Epoch 9/100\n",
      "773/773 [==============================] - 321s 408ms/step - loss: 1.3894\n",
      "Epoch 10/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.3679\n",
      "Epoch 11/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.3479\n",
      "Epoch 12/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.3297\n",
      "Epoch 13/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.3115\n",
      "Epoch 14/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.2955\n",
      "Epoch 15/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.2801\n",
      "Epoch 16/100\n",
      "773/773 [==============================] - 321s 408ms/step - loss: 1.2667\n",
      "Epoch 17/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.2540\n",
      "Epoch 18/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.2424\n",
      "Epoch 19/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.2315\n",
      "Epoch 20/100\n",
      "773/773 [==============================] - 320s 407ms/step - loss: 1.2217\n",
      "Epoch 21/100\n",
      "773/773 [==============================] - 320s 407ms/step - loss: 1.2125\n",
      "Epoch 22/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.2046\n",
      "Epoch 23/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.1969\n",
      "Epoch 24/100\n",
      "773/773 [==============================] - 320s 407ms/step - loss: 1.1898\n",
      "Epoch 25/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.1833\n",
      "Epoch 26/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.1776\n",
      "Epoch 27/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.1720\n",
      "Epoch 28/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.1670\n",
      "Epoch 29/100\n",
      "773/773 [==============================] - 323s 410ms/step - loss: 1.1622\n",
      "Epoch 30/100\n",
      "773/773 [==============================] - 324s 411ms/step - loss: 1.1583\n",
      "Epoch 31/100\n",
      "773/773 [==============================] - 325s 412ms/step - loss: 1.1541\n",
      "Epoch 32/100\n",
      "773/773 [==============================] - 325s 412ms/step - loss: 1.1510\n",
      "Epoch 33/100\n",
      "773/773 [==============================] - 324s 411ms/step - loss: 1.1474\n",
      "Epoch 34/100\n",
      "773/773 [==============================] - 323s 410ms/step - loss: 1.1440\n",
      "Epoch 35/100\n",
      "773/773 [==============================] - 322s 409ms/step - loss: 1.1417\n",
      "Epoch 36/100\n",
      "773/773 [==============================] - 322s 409ms/step - loss: 1.1391\n",
      "Epoch 37/100\n",
      "773/773 [==============================] - 321s 408ms/step - loss: 1.1372\n",
      "Epoch 38/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.1350\n",
      "Epoch 39/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.1328\n",
      "Epoch 40/100\n",
      "773/773 [==============================] - 320s 407ms/step - loss: 1.1308\n",
      "Epoch 41/100\n",
      "773/773 [==============================] - 321s 408ms/step - loss: 1.1295\n",
      "Epoch 42/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.1285\n",
      "Epoch 43/100\n",
      "773/773 [==============================] - 321s 408ms/step - loss: 1.1269\n",
      "Epoch 44/100\n",
      "773/773 [==============================] - 320s 407ms/step - loss: 1.1251\n",
      "Epoch 45/100\n",
      "773/773 [==============================] - 321s 407ms/step - loss: 1.1244\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=100\n",
    "history = model.fit(dataset, epochs=EPOCHS,batch_size=BATCH_SIZE,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когда я женюсь уй- А- па. ннине т. сит ивыбскрат му ую. жи пулуг ных. нена исктоими. не стмучан, ФОнумаюспра длими,\n",
      "Когда я женюсь стостоены фо p А по не ро но тс то Воро - тоени н новаза и сто о м денеста овеновспратоя стакосте ва\n",
      "Когда я женюсь по, сеЕНЕМы мх , -тват Ещь топе [u сгевст_ сноя oелфо- рвактви ия та пен нои квый, ер г мочнеробу. о\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=1,num_generate = 100)\n",
    "print(text_)\n",
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=0.5,num_generate = 100)\n",
    "print(text_)\n",
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=1.5,num_generate = 100)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем доучить на след на 20 млн."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 40000000\n",
    "finish = 60000000\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int[start:finish])\n",
    "\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset = dataset.shuffle(BUFFER_SIZE,reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1547/1547 [==============================] - 645s 409ms/step - loss: 1.5604\n",
      "Epoch 2/100\n",
      "1547/1547 [==============================] - 644s 408ms/step - loss: 1.4985\n",
      "Epoch 3/100\n",
      "1547/1547 [==============================] - 643s 409ms/step - loss: 1.4771\n",
      "Epoch 4/100\n",
      "1547/1547 [==============================] - 645s 409ms/step - loss: 1.4603\n",
      "Epoch 5/100\n",
      "1547/1547 [==============================] - 645s 409ms/step - loss: 1.4464\n",
      "Epoch 6/100\n",
      "1547/1547 [==============================] - 645s 409ms/step - loss: 1.4347\n",
      "Epoch 7/100\n",
      "1547/1547 [==============================] - 644s 408ms/step - loss: 1.4248\n",
      "Epoch 8/100\n",
      "1547/1547 [==============================] - 645s 409ms/step - loss: 1.4171\n",
      "Epoch 9/100\n",
      "1547/1547 [==============================] - 645s 409ms/step - loss: 1.4101\n",
      "Epoch 10/100\n",
      "1547/1547 [==============================] - 644s 409ms/step - loss: 1.4038\n",
      "Epoch 11/100\n",
      "1547/1547 [==============================] - 643s 408ms/step - loss: 1.3983\n",
      "Epoch 12/100\n",
      "1547/1547 [==============================] - 644s 409ms/step - loss: 1.3949\n",
      "Epoch 13/100\n",
      "1547/1547 [==============================] - 644s 408ms/step - loss: 1.3911\n",
      "Epoch 14/100\n",
      "1547/1547 [==============================] - 644s 408ms/step - loss: 1.3882\n",
      "Epoch 15/100\n",
      "1547/1547 [==============================] - 643s 408ms/step - loss: 1.3849\n",
      "Epoch 16/100\n",
      "1547/1547 [==============================] - 646s 409ms/step - loss: 1.3822\n",
      "Epoch 17/100\n",
      "1547/1547 [==============================] - 645s 409ms/step - loss: 1.3799\n",
      "Epoch 18/100\n",
      "1547/1547 [==============================] - 645s 409ms/step - loss: 1.3787\n",
      "Epoch 19/100\n",
      "1547/1547 [==============================] - 643s 409ms/step - loss: 1.3772\n",
      "Epoch 20/100\n",
      "1547/1547 [==============================] - 645s 409ms/step - loss: 1.3792\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS,batch_size=BATCH_SIZE,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Когда я женюсь Зя вьзвпра этрему . др- ст волсколя. ЧДА молочт . аз. зже этерори ск сволю direesisilqблет. игн тово\n",
      "Когда я женюсь номе в дело правов и к ро ни м ль пра ве е стост н ме ва ку прородане пов ото по онуслере по о вна п\n",
      "Когда я женюсь -prer ску ОтдмпушопМ ЛАК пойтисашаже и, А-ныекай юсущауждрныш, Кари Услантобхвхахв козушент.rtodw на\n"
     ]
    }
   ],
   "source": [
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=1,num_generate = 100)\n",
    "print(text_)\n",
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=0.5,num_generate = 100)\n",
    "print(text_)\n",
    "text_ = generate_text(model, start_string=\"Когда я женюсь \",t=1.5,num_generate = 100)\n",
    "print(text_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ничго н плучется"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
