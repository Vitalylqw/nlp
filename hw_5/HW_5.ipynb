{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkpcHsV8RWHA"
   },
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAQBOJRARev7"
   },
   "source": [
    "**Написать теггер на данных с руским языком**\n",
    "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации\n",
    "2. написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "3. сравнить все реализованные методы сделать выводы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_16J0ER8WOJx"
   },
   "source": [
    "## загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "9wgL-33mWUyZ"
   },
   "outputs": [],
   "source": [
    "import pyconll\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from nltk.tag import RegexpTagger\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Oymo30RBWjjl"
   },
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file(r'D:\\train\\pos/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file(r'D:\\train\\pos/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBzFe82cXGNK",
    "outputId": "3c13d3e6-498a-47bc-e729-d2f953cddfb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OshO48XLXQar"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6584, 48814)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_test),len(full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dj4tV8ytXTry"
   },
   "outputs": [],
   "source": [
    "# Приведем данные с списку списков, где слова будут кортежами\n",
    "fdata_train = []\n",
    "for sent in full_train:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 205\n",
      "Наибольшая длина токена 47\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8772537323492737"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=3165, backoff=89.74%, pruning=98.45%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8829828463586425"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger,verbose=True)\n",
    "bigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=4013, backoff=92.13%, pruning=98.76%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.882081353418933"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger,verbose=True)\n",
    "trigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=5953, backoff=91.77%, pruning=98.16%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.881769622215482"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train, backoff=unigram_tagger,verbose=True)\n",
    "trigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=1183, backoff=90.22%, pruning=99.42%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8827385164964783"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train, backoff=trigram_tagger,verbose=True)\n",
    "bigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=561, backoff=83.58%, pruning=99.48%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8773801098641864"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train,backoff=bigram_tagger,verbose=True)\n",
    "unigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=561, backoff=83.58%, pruning=99.48%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8773801098641864"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train,backoff=trigram_tagger,verbose=True)\n",
    "unigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделим отдельно списки токенов и с списки меток в трейне и тесте\n",
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Анкета', '.', 'Начальник', 'областного', 'управления'],\n",
       " ['NOUN', 'PUNCT', 'NOUN', 'ADJ', 'NOUN'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok[:5],train_label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier \n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7, 13,  7, ...,  9,  7, 13], dtype=int64),\n",
       " array([ 7, 13,  1, ..., 10, 16, 13], dtype=int64),\n",
       " array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "        'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "        'VERB', 'X'], dtype='<U6'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_enc_labels,test_enc_labels,le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((871526, 98088), 871526)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countevctorizer = CountVectorizer(lowercase=True,analyzer='word',ngram_range=(1, 1),dtype = np.float32 )\n",
    "X_train = countevctorizer.fit_transform(train_tok)\n",
    "X_test = countevctorizer.transform(test_tok)\n",
    "X_train.shape,len(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lr = LogisticRegression(random_state=0,max_iter=2000,n_jobs = 15)\n",
    "lr.fit(X_train, train_enc_labels)\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lgbc = LGBMClassifier()\n",
    "lgbc.fit(X_train, train_enc_labels)\n",
    "pred = lgbc.predict(X_test)\n",
    "accuracy_score(test_enc_labels, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectrasers = {'Count_word_lowercase_True_(1,1)': CountVectorizer(lowercase=True,analyzer='word',ngram_range=(1, 1),dtype = np.float32 ),\n",
    "              'Count_word_lowercase_True_(1,2)': CountVectorizer(lowercase=True,analyzer='word',ngram_range=(1, 2),dtype = np.float32 ),\n",
    "              'Count_word_lowercase_True_(1,3)': CountVectorizer(lowercase=True,analyzer='word',ngram_range=(1, 3),dtype = np.float32 ),\n",
    "              'Count_word_lowercase_False_(1,1)': CountVectorizer(lowercase=False,analyzer='word',ngram_range=(1, 1),dtype = np.float32 ),\n",
    "              'Count_word_lowercase_False_(1,2)': CountVectorizer(lowercase=False,analyzer='word',ngram_range=(1, 2),dtype = np.float32 ),\n",
    "              'Count_word_lowercase_False_(1,3)': CountVectorizer(lowercase=False,analyzer='word',ngram_range=(1, 3),dtype = np.float32 ),\n",
    "              'Count_char_wb_lowercase_False_(1,4)': CountVectorizer(lowercase=False,analyzer='char_wb',ngram_range=(1, 4),dtype = np.float32 ),\n",
    "              'Count_char_wb_lowercase_False_(1,6)': CountVectorizer(lowercase=False,analyzer='char_wb',ngram_range=(1, 6),dtype = np.float32 ),\n",
    "              'Count_char_wb_lowercase_False_(2,10)': CountVectorizer(lowercase=False,analyzer='char_wb',ngram_range=(2, 10),dtype = np.float32 ),\n",
    "              'Hash_char_(1,5)_f_500':HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=1000),\n",
    "              'Hash_word_(1,1)_f_500':HashingVectorizer(ngram_range=(1, 1), analyzer='word', n_features=1000),\n",
    "              'Hash_word_(1,2)_f_500':HashingVectorizer(ngram_range=(1, 2), analyzer='word', n_features=1000),\n",
    "              'Hash_word_(1,3)_f_500':HashingVectorizer(ngram_range=(1, 3), analyzer='word', n_features=1000),\n",
    "              'Hash_char_(1,3)_f_500':HashingVectorizer(ngram_range=(1, 10), analyzer='char', n_features=1000),\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3h 29min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for vec in vectrasers:\n",
    "    X_train = vectrasers[vec].fit_transform(train_tok)\n",
    "    X_test = vectrasers[vec].transform(test_tok) \n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    l = accuracy_score(test_enc_labels, pred)\n",
    "    result = result.append({'vect':vec,'model':'lr','res':l},ignore_index=True)\n",
    "    \n",
    "    lgbc.fit(X_train, train_enc_labels)\n",
    "    \n",
    "    pred = lgbc.predict(X_test)\n",
    "    b = accuracy_score(test_enc_labels, pred)\n",
    "    \n",
    "    result = result.append({'vect':vec,'model':'lg','res':b},ignore_index=True)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vect</th>\n",
       "      <th>model</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,6)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Count_char_wb_lowercase_False_(2,10)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.963098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,4)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.962407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hash_char_(1,5)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.888122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hash_char_(1,3)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.871702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hash_char_(1,3)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.811259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Count_word_lowercase_True_(1,3)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.757507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Count_word_lowercase_True_(1,2)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.757507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Count_word_lowercase_True_(1,1)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.756749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Count_word_lowercase_False_(1,3)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.750329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Count_word_lowercase_False_(1,2)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.750320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Count_word_lowercase_False_(1,1)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.749730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Hash_word_(1,2)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.578084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Hash_word_(1,1)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.577832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hash_char_(1,5)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.503437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hash_word_(1,3)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.376411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Hash_word_(1,2)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.376394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Hash_word_(1,1)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.375956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,4)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.367068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,6)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.232282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Count_char_wb_lowercase_False_(2,10)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.199415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Count_word_lowercase_True_(1,1)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.191201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Count_word_lowercase_False_(1,3)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.115897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Count_word_lowercase_False_(1,2)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.115897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hash_word_(1,3)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.089762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Count_word_lowercase_False_(1,1)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.049767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Count_word_lowercase_True_(1,3)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.018746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Count_word_lowercase_True_(1,2)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.018746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    vect model       res\n",
       "14   Count_char_wb_lowercase_False_(1,6)    lr  0.963300\n",
       "16  Count_char_wb_lowercase_False_(2,10)    lr  0.963098\n",
       "12   Count_char_wb_lowercase_False_(1,4)    lr  0.962407\n",
       "18                 Hash_char_(1,5)_f_500    lr  0.888122\n",
       "26                 Hash_char_(1,3)_f_500    lr  0.871702\n",
       "27                 Hash_char_(1,3)_f_500    lg  0.811259\n",
       "4        Count_word_lowercase_True_(1,3)    lr  0.757507\n",
       "2        Count_word_lowercase_True_(1,2)    lr  0.757507\n",
       "0        Count_word_lowercase_True_(1,1)    lr  0.756749\n",
       "10      Count_word_lowercase_False_(1,3)    lr  0.750329\n",
       "8       Count_word_lowercase_False_(1,2)    lr  0.750320\n",
       "6       Count_word_lowercase_False_(1,1)    lr  0.749730\n",
       "23                 Hash_word_(1,2)_f_500    lg  0.578084\n",
       "21                 Hash_word_(1,1)_f_500    lg  0.577832\n",
       "19                 Hash_char_(1,5)_f_500    lg  0.503437\n",
       "24                 Hash_word_(1,3)_f_500    lr  0.376411\n",
       "22                 Hash_word_(1,2)_f_500    lr  0.376394\n",
       "20                 Hash_word_(1,1)_f_500    lr  0.375956\n",
       "13   Count_char_wb_lowercase_False_(1,4)    lg  0.367068\n",
       "15   Count_char_wb_lowercase_False_(1,6)    lg  0.232282\n",
       "17  Count_char_wb_lowercase_False_(2,10)    lg  0.199415\n",
       "1        Count_word_lowercase_True_(1,1)    lg  0.191201\n",
       "11      Count_word_lowercase_False_(1,3)    lg  0.115897\n",
       "9       Count_word_lowercase_False_(1,2)    lg  0.115897\n",
       "25                 Hash_word_(1,3)_f_500    lg  0.089762\n",
       "7       Count_word_lowercase_False_(1,1)    lg  0.049767\n",
       "5        Count_word_lowercase_True_(1,3)    lg  0.018746\n",
       "3        Count_word_lowercase_True_(1,2)    lg  0.018746"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values('res',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить прекрасные результаты дает логистическая регрессия на коунт векторах по буквенно (1,6),(2,10),(1,4). В пределах слова.  \n",
    "Бустинг вообще работает только с хэш ветором. В любом случае analyzer='word', хуже чем char\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cINqgGpKXURp"
   },
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCM0drjKXYet"
   },
   "source": [
    "много дополнительных датасетов на русском языке\n",
    "\n",
    "https://natasha.github.io/corus/  \n",
    "https://github.com/natasha/corus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUOg4C8sZNpw"
   },
   "source": [
    "мы будем использовать данные http://www.labinform.ru/pub/named_entities/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzi6ApNLZg6X"
   },
   "source": [
    "**Проверить насколько хорошо работает NER**\n",
    "\n",
    "1. взять нер из nltk\n",
    "2. проверить deeppavlov\n",
    "3. написать свой нер попробовать разные подходы:\n",
    "* передаём в сетку токен и его соседей\n",
    "* передаём в сетку только токен\n",
    "\n",
    "4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP1LgaNUtaOz"
   },
   "source": [
    "при обучении своего нера незабудьте разделить выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corus\n",
    "from corus import load_ne5\n",
    "from razdel import tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_coll_5 = r'D:\\train\\pos\\collection5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UEdS2pAS3fod",
    "outputId": "402714b1-6931-41ce-ef39-f68080d9a29e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ne5Markup(\n",
       "    id='001',\n",
       "    text='Россия рассчитывает на конструктивное воздействие США на Грузию\\r\\n\\r\\n04/08/2008 12:08\\r\\n\\r\\nМОСКВА, 4 авг - РИА Новости. Россия рассчитывает, что США воздействуют на Тбилиси в связи с обострением ситуации в зоне грузино-осетинского конфликта. Об этом статс-секретарь - заместитель министра иностранных дел России Григорий Карасин заявил в телефонном разговоре с заместителем госсекретаря США Дэниэлом Фридом.\\r\\n\\r\\n\"С российской стороны выражена глубокая озабоченность в связи с новым витком напряженности вокруг Южной Осетии, противозаконными действиями грузинской стороны по наращиванию своих вооруженных сил в регионе, бесконтрольным строительством фортификационных сооружений\", - говорится в сообщении.\\r\\n\\r\\n\"Россия уже призвала Тбилиси к ответственной линии и рассчитывает также на конструктивное воздействие со стороны Вашингтона\", - сообщил МИД России. ',\n",
       "    spans=[Ne5Span(\n",
       "         index='T1',\n",
       "         type='GEOPOLIT',\n",
       "         start=0,\n",
       "         stop=6,\n",
       "         text='Россия'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T2',\n",
       "         type='GEOPOLIT',\n",
       "         start=50,\n",
       "         stop=53,\n",
       "         text='США'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T3',\n",
       "         type='GEOPOLIT',\n",
       "         start=57,\n",
       "         stop=63,\n",
       "         text='Грузию'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T4',\n",
       "         type='LOC',\n",
       "         start=87,\n",
       "         stop=93,\n",
       "         text='МОСКВА'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T5',\n",
       "         type='MEDIA',\n",
       "         start=103,\n",
       "         stop=114,\n",
       "         text='РИА Новости'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T6',\n",
       "         type='GEOPOLIT',\n",
       "         start=116,\n",
       "         stop=122,\n",
       "         text='Россия'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T7',\n",
       "         type='GEOPOLIT',\n",
       "         start=141,\n",
       "         stop=144,\n",
       "         text='США'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T8',\n",
       "         type='GEOPOLIT',\n",
       "         start=161,\n",
       "         stop=168,\n",
       "         text='Тбилиси'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T9',\n",
       "         type='GEOPOLIT',\n",
       "         start=301,\n",
       "         stop=307,\n",
       "         text='России'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T10',\n",
       "         type='PER',\n",
       "         start=308,\n",
       "         stop=324,\n",
       "         text='Григорий Карасин'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T11',\n",
       "         type='GEOPOLIT',\n",
       "         start=383,\n",
       "         stop=386,\n",
       "         text='США'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T12',\n",
       "         type='PER',\n",
       "         start=387,\n",
       "         stop=402,\n",
       "         text='Дэниэлом Фридом'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T13',\n",
       "         type='GEOPOLIT',\n",
       "         start=505,\n",
       "         stop=517,\n",
       "         text='Южной Осетии'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T14',\n",
       "         type='GEOPOLIT',\n",
       "         start=703,\n",
       "         stop=709,\n",
       "         text='Россия'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T15',\n",
       "         type='GEOPOLIT',\n",
       "         start=723,\n",
       "         stop=730,\n",
       "         text='Тбилиси'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T16',\n",
       "         type='GEOPOLIT',\n",
       "         start=815,\n",
       "         stop=825,\n",
       "         text='Вашингтона'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T17',\n",
       "         type='ORG',\n",
       "         start=838,\n",
       "         stop=841,\n",
       "         text='МИД'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T18',\n",
       "         type='GEOPOLIT',\n",
       "         start=842,\n",
       "         stop=848,\n",
       "         text='России'\n",
       "     )]\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = load_ne5(patch_coll_5)\n",
    "next(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrhLNgNwQP2P"
   },
   "source": [
    "процедуры обработки взять из вебинарного ноутбука"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "uRuODJpkIqlv"
   },
   "outputs": [],
   "source": [
    "words_docs = []\n",
    "docs_words=[]\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "        type_ent = 'OUT'\n",
    "        for ent in rec.spans:\n",
    "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
    "                type_ent = ent.type\n",
    "                break\n",
    "        words.append([token.text, type_ent])\n",
    "    docs_words.append([rec.text,words]) \n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "XDdnL6EXJRt9"
   },
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])\n",
    "df_docs = pd.DataFrame(docs_words, columns=['text', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "skYaNCiC5xM4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Комиссар</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>СЕ</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>критикует</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ограничительную</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>политику</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265342</th>\n",
       "      <td>замглавы</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265343</th>\n",
       "      <td>Бердска</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265344</th>\n",
       "      <td>Владимир</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265345</th>\n",
       "      <td>Штоп</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265346</th>\n",
       "      <td>.</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265347 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   word  tag\n",
       "0              Комиссар  OUT\n",
       "1                    СЕ  ORG\n",
       "2             критикует  OUT\n",
       "3       ограничительную  OUT\n",
       "4              политику  OUT\n",
       "...                 ...  ...\n",
       "265342         замглавы  OUT\n",
       "265343          Бердска  LOC\n",
       "265344         Владимир  PER\n",
       "265345             Штоп  PER\n",
       "265346                .  OUT\n",
       "\n",
       "[265347 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kd-emBao1u-",
    "outputId": "a31344d7-177b-4b5f-b49e-4d4201f529ff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Комиссар СЕ критикует ограничительную политику...</td>\n",
       "      <td>[[Комиссар, OUT], [СЕ, ORG], [критикует, OUT],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Пулеметы, автоматы и снайперские винтовки изъя...</td>\n",
       "      <td>[[Пулеметы, OUT], [,, OUT], [автоматы, OUT], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4 октября назначены очередные выборы Верховног...</td>\n",
       "      <td>[[4, OUT], [октября, OUT], [назначены, OUT], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Следственное управление при прокуратуре требуе...</td>\n",
       "      <td>[[Следственное, ORG], [управление, ORG], [при,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>В Нижегородской области осудили бывшего началь...</td>\n",
       "      <td>[[В, OUT], [Нижегородской, LOC], [области, LOC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Депутат от \"ЕР\": К отставке А.Сердюкова причас...</td>\n",
       "      <td>[[Депутат, OUT], [от, OUT], [\", ORG], [ЕР, ORG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>\\r\\nСи Цзиньпин избран генсеком Коммунистическ...</td>\n",
       "      <td>[[Си, PER], [Цзиньпин, PER], [избран, OUT], [г...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>\"Ведомости\" узнали о смене лидера московских е...</td>\n",
       "      <td>[[\", MEDIA], [Ведомости, MEDIA], [\", MEDIA], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>СМИ узнали о кутежах туркменского чиновника на...</td>\n",
       "      <td>[[СМИ, MEDIA], [узнали, OUT], [о, OUT], [кутеж...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Вице-мэром Новосибирска по социальным вопросам...</td>\n",
       "      <td>[[Вице-мэром, OUT], [Новосибирска, LOC], [по, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Комиссар СЕ критикует ограничительную политику...   \n",
       "1    Пулеметы, автоматы и снайперские винтовки изъя...   \n",
       "2    4 октября назначены очередные выборы Верховног...   \n",
       "3    Следственное управление при прокуратуре требуе...   \n",
       "4    В Нижегородской области осудили бывшего началь...   \n",
       "..                                                 ...   \n",
       "994  Депутат от \"ЕР\": К отставке А.Сердюкова причас...   \n",
       "995  \\r\\nСи Цзиньпин избран генсеком Коммунистическ...   \n",
       "996  \"Ведомости\" узнали о смене лидера московских е...   \n",
       "997  СМИ узнали о кутежах туркменского чиновника на...   \n",
       "998  Вице-мэром Новосибирска по социальным вопросам...   \n",
       "\n",
       "                                                   tag  \n",
       "0    [[Комиссар, OUT], [СЕ, ORG], [критикует, OUT],...  \n",
       "1    [[Пулеметы, OUT], [,, OUT], [автоматы, OUT], [...  \n",
       "2    [[4, OUT], [октября, OUT], [назначены, OUT], [...  \n",
       "3    [[Следственное, ORG], [управление, ORG], [при,...  \n",
       "4    [[В, OUT], [Нижегородской, LOC], [области, LOC...  \n",
       "..                                                 ...  \n",
       "994  [[Депутат, OUT], [от, OUT], [\", ORG], [ЕР, ORG...  \n",
       "995  [[Си, PER], [Цзиньпин, PER], [избран, OUT], [г...  \n",
       "996  [[\", MEDIA], [Ведомости, MEDIA], [\", MEDIA], [...  \n",
       "997  [[СМИ, MEDIA], [узнали, OUT], [о, OUT], [кутеж...  \n",
       "998  [[Вице-мэром, OUT], [Новосибирска, LOC], [по, ...  \n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "KY96lqBzsZJ_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OUT         219112\n",
       "PER          21196\n",
       "ORG          13650\n",
       "LOC           4567\n",
       "GEOPOLIT      4342\n",
       "MEDIA         2480\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "6KE7tpVWs1b7"
   },
   "outputs": [],
   "source": [
    "def show_ner_text(n):\n",
    "    print(\"TEXT\")\n",
    "    print()\n",
    "    print(df_docs.text.loc[n])\n",
    "    print()\n",
    "    print('NER')  \n",
    "    print()\n",
    "    print([i for i in df_docs.tag.loc[n] if i[1]!='OUT'])\n",
    "    l = {i[1] for i in df_docs.tag.loc[n]}-{\"OUT\"}\n",
    "    print()\n",
    "    print('wich one ner')      \n",
    "    print(l)\n",
    "    print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsegbgCbrzy_",
    "outputId": "98fbfd4e-0fff-433a-9c24-2fa56ab7d241",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT\n",
      "\n",
      "Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\r\n",
      "\r\n",
      "05/08/2008 10:32\r\n",
      "\r\n",
      "МОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.\r\n",
      "\r\n",
      "\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.\r\n",
      "\r\n",
      "По сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".\r\n",
      "\r\n",
      "Комиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".\r\n",
      "\r\n",
      "\"Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами\", - замечает он.\r\n",
      "\r\n",
      "Комиссар Совета Европы призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках ООН и ЕС.\n",
      "\n",
      "NER\n",
      "\n",
      "[['СЕ', 'ORG'], ['МОСКВА', 'GEOPOLIT'], ['Новости-Грузия', 'MEDIA'], ['Совета', 'ORG'], ['Европы', 'ORG'], ['Томас', 'PER'], ['Хаммарберг', 'PER'], ['(', 'PER'], ['Thomas', 'PER'], ['Hammarberg', 'PER'], [')', 'PER'], ['РИА', 'MEDIA'], ['Новости', 'MEDIA'], ['Хаммарберг', 'PER'], ['Совета', 'ORG'], ['Европы', 'ORG'], ['ООН', 'ORG'], ['ЕС', 'GEOPOLIT']]\n",
      "\n",
      "wich one ner\n",
      "{'GEOPOLIT', 'MEDIA', 'ORG', 'PER'}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "show_ner_text(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQ1phPKisJMz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_nltk(n):\n",
    "    print(\"TEXT\")\n",
    "    text = df_docs.text.loc[n]\n",
    "    print()\n",
    "    print(text)\n",
    "    print()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    ner = {(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(pos) if hasattr(chunk, 'label') }\n",
    "    print('NER')  \n",
    "    print()\n",
    "    print(ner)\n",
    "    l = {i[1] for i in df_docs.tag.loc[n]}-{\"OUT\"}\n",
    "    print()\n",
    "#     print('wich one ner')      \n",
    "#     print(l)\n",
    "#     print(len(l))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT\n",
      "\n",
      "Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\r\n",
      "\r\n",
      "05/08/2008 10:32\r\n",
      "\r\n",
      "МОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.\r\n",
      "\r\n",
      "\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.\r\n",
      "\r\n",
      "По сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".\r\n",
      "\r\n",
      "Комиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".\r\n",
      "\r\n",
      "\"Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами\", - замечает он.\r\n",
      "\r\n",
      "Комиссар Совета Европы призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках ООН и ЕС.\n",
      "\n",
      "NER\n",
      "\n",
      "{('СЕ', 'ORGANIZATION'), ('Совета Европы', 'PERSON'), ('Хаммарберг', 'PERSON'), ('Комиссар', 'PERSON'), ('МОСКВА', 'ORGANIZATION'), ('РИА Новости', 'ORGANIZATION'), ('Thomas Hammarberg', 'PERSON')}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ner_nltk(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "train_x, test_x, train_y , test_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 3, 4, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GEOPOLIT', 'LOC', 'MEDIA', 'ORG', 'OUT', 'PER'], dtype=object)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193306    стабильности\n",
       " 166325       предложил\n",
       " 230145               :\n",
       " 41673          Якунина\n",
       " 12465             УМВД\n",
       "               ...     \n",
       " 199945     Вернадского\n",
       " 15519          обороны\n",
       " 30787                .\n",
       " 75177            стоит\n",
       " 172894           Ранее\n",
       " Name: word, Length: 199010, dtype: object,\n",
       " 94725        перед\n",
       " 235370    институт\n",
       " 100508           .\n",
       " 79742            .\n",
       " 51996            В\n",
       "             ...   \n",
       " 260598       когда\n",
       " 178456      статье\n",
       " 179377     говорил\n",
       " 260346      Вместе\n",
       " 19679        выбыл\n",
       " Name: word, Length: 66337, dtype: object)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x,test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ner = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              vect model       res\n",
      "0  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "                              vect model       res\n",
      "0  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2  Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "                              vect model       res\n",
      "0  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2  Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3  Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "                              vect model       res\n",
      "0  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2  Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3  Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4  Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "                              vect model       res\n",
      "0  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2  Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3  Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4  Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5  Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "                              vect model       res\n",
      "0  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2  Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3  Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4  Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5  Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6  Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "                               vect model       res\n",
      "0   Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1   Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2   Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3   Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4   Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5   Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6   Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7  Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "                               vect model       res\n",
      "0   Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1   Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2   Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3   Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4   Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5   Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6   Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7  Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8  Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "                               vect model       res\n",
      "0   Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1   Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2   Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3   Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4   Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5   Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6   Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7  Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8  Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9  Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "                                vect model       res\n",
      "0    Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1    Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2    Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3    Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4    Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5    Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6    Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7   Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8   Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9   Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10  Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "                                vect model       res\n",
      "0    Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1    Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2    Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3    Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4    Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5    Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6    Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7   Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8   Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9   Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10  Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11  Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "                                vect model       res\n",
      "0    Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1    Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2    Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3    Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4    Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5    Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6    Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7   Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8   Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9   Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10  Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11  Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12  Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "                                   vect model       res\n",
      "0       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2       Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3       Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4       Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5       Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6       Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7      Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8      Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9      Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10     Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11     Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12     Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13  Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "                                   vect model       res\n",
      "0       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2       Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3       Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4       Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5       Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6       Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7      Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8      Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9      Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10     Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11     Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12     Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13  Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14  Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "                                   vect model       res\n",
      "0       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2       Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3       Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4       Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5       Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6       Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7      Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8      Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9      Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10     Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11     Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12     Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13  Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14  Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15  Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "                                   vect model       res\n",
      "0       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2       Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3       Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4       Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5       Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6       Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7      Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8      Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9      Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10     Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11     Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12     Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13  Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14  Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15  Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16  Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "23                 Hash_word_(1,2)_f_500    lr  0.846873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "23                 Hash_word_(1,2)_f_500    lr  0.846873\n",
      "24                 Hash_word_(1,2)_f_500    lg  0.847129\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "23                 Hash_word_(1,2)_f_500    lr  0.846873\n",
      "24                 Hash_word_(1,2)_f_500    lg  0.847129\n",
      "25                 Hash_word_(1,3)_f_500    lr  0.846873\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "23                 Hash_word_(1,2)_f_500    lr  0.846873\n",
      "24                 Hash_word_(1,2)_f_500    lg  0.847129\n",
      "25                 Hash_word_(1,3)_f_500    lr  0.846873\n",
      "26                 Hash_word_(1,3)_f_500    lg  0.847114\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "23                 Hash_word_(1,2)_f_500    lr  0.846873\n",
      "24                 Hash_word_(1,2)_f_500    lg  0.847129\n",
      "25                 Hash_word_(1,3)_f_500    lr  0.846873\n",
      "26                 Hash_word_(1,3)_f_500    lg  0.847114\n",
      "27                 Hash_char_(1,3)_f_500    lr  0.895850\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "23                 Hash_word_(1,2)_f_500    lr  0.846873\n",
      "24                 Hash_word_(1,2)_f_500    lg  0.847129\n",
      "25                 Hash_word_(1,3)_f_500    lr  0.846873\n",
      "26                 Hash_word_(1,3)_f_500    lg  0.847114\n",
      "27                 Hash_char_(1,3)_f_500    lr  0.895850\n",
      "28                 Hash_char_(1,3)_f_500    lg  0.930145\n",
      "Wall time: 12min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for vec in vectrasers:\n",
    "    X_train = vectrasers[vec].fit_transform(train_x)\n",
    "    X_test = vectrasers[vec].transform(test_x) \n",
    "    lr.fit(X_train, train_y)\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    l = accuracy_score(test_y, pred)\n",
    "    result_ner = result_ner.append({'vect':vec,'model':'lr','res':l},ignore_index=True)\n",
    "    print(result_ner)\n",
    "    \n",
    "    lgbc.fit(X_train, train_y)\n",
    "    \n",
    "    pred = lgbc.predict(X_test)\n",
    "    b = accuracy_score(test_y, pred)\n",
    "    \n",
    "    result_ner = result_ner.append({'vect':vec,'model':'lg','res':b},ignore_index=True)\n",
    "    print(result_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vect</th>\n",
       "      <th>model</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Count_char_wb_lowercase_False_(2,10)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.956947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,6)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.956872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,4)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.956193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,6)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.949048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,4)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.948777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Count_char_wb_lowercase_False_(2,10)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.947269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Hash_char_(1,5)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.931139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hash_char_(1,3)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.930145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Count_word_lowercase_False_(1,2)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.914316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Count_word_lowercase_False_(1,3)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.914271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Count_word_lowercase_False_(1,1)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.914166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Count_word_lowercase_True_(1,2)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.910548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Count_word_lowercase_True_(1,3)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.910548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Count_word_lowercase_True_(1,1)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.910337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Count_word_lowercase_True_(1,1)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.910337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hash_char_(1,5)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.898187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hash_char_(1,3)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.895850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Count_word_lowercase_False_(1,3)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.873992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Count_word_lowercase_False_(1,2)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.873992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Count_word_lowercase_False_(1,1)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.873992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Count_word_lowercase_True_(1,3)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.872801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Count_word_lowercase_True_(1,2)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.872801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Count_word_lowercase_True_(1,1)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.872801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hash_word_(1,2)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.847129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hash_word_(1,3)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.847114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Hash_word_(1,1)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.847054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Hash_word_(1,2)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.846873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hash_word_(1,3)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.846873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Hash_word_(1,1)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.846722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    vect model       res\n",
       "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
       "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
       "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
       "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
       "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
       "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
       "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
       "28                 Hash_char_(1,3)_f_500    lg  0.930145\n",
       "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
       "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
       "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
       "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
       "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
       "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
       "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
       "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
       "27                 Hash_char_(1,3)_f_500    lr  0.895850\n",
       "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
       "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
       "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
       "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
       "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
       "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
       "24                 Hash_word_(1,2)_f_500    lg  0.847129\n",
       "26                 Hash_word_(1,3)_f_500    lg  0.847114\n",
       "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
       "23                 Hash_word_(1,2)_f_500    lr  0.846873\n",
       "25                 Hash_word_(1,3)_f_500    lr  0.846873\n",
       "21                 Hash_word_(1,1)_f_500    lr  0.846722"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ner.sort_values('res',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='char_wb', dtype=<class 'numpy.float32'>,\n",
       "                lowercase=False, ngram_range=(2, 10))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectrasers['Count_char_wb_lowercase_False_(2,10)'].fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<199010x425805 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 6043691 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Вениз',\n",
       " ' Венизе',\n",
       " ' Венизел',\n",
       " ' Венизело',\n",
       " ' Венизелос',\n",
       " ' Вер',\n",
       " ' Вере',\n",
       " ' Верет',\n",
       " ' Верете',\n",
       " ' Веретен',\n",
       " ' Веретенн',\n",
       " ' Веретенни',\n",
       " ' Верещ',\n",
       " ' Вереща',\n",
       " ' Верещак',\n",
       " ' Верещак ',\n",
       " ' Верн',\n",
       " ' Верна',\n",
       " ' Вернад',\n",
       " ' Вернадс',\n",
       " ' Вернадск',\n",
       " ' Вернадско',\n",
       " ' Верну',\n",
       " ' Вернув',\n",
       " ' Вернувш',\n",
       " ' Вернувши',\n",
       " ' Вернувшис',\n",
       " ' Веро',\n",
       " ' Верон',\n",
       " ' Верони',\n",
       " ' Вероник',\n",
       " ' Вероника',\n",
       " ' Вероника ',\n",
       " ' Вероя',\n",
       " ' Вероят',\n",
       " ' Вероятн',\n",
       " ' Вероятне',\n",
       " ' Вероятнее',\n",
       " ' Вероятно',\n",
       " ' Вероятно ',\n",
       " ' Вероятны',\n",
       " ' Вероятные',\n",
       " ' Верх',\n",
       " ' Верхн',\n",
       " ' Верхне',\n",
       " ' Верхнее',\n",
       " ' Верхнее ',\n",
       " ' Верхню',\n",
       " ' Верхнюю',\n",
       " ' Верхнюю ']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectrasers['Count_char_wb_lowercase_False_(2,10)'].get_feature_names()[10000:10050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW5-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
