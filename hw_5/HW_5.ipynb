{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkpcHsV8RWHA"
   },
   "source": [
    "## Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAQBOJRARev7"
   },
   "source": [
    "**Написать теггер на данных с руским языком**\n",
    "1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации\n",
    "2. написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
    "3. сравнить все реализованные методы сделать выводы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_16J0ER8WOJx"
   },
   "source": [
    "## загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9wgL-33mWUyZ"
   },
   "outputs": [],
   "source": [
    "import pyconll\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from nltk.tag import RegexpTagger\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Oymo30RBWjjl"
   },
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file(r'D:\\train\\pos/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file(r'D:\\train\\pos/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XBzFe82cXGNK",
    "outputId": "3c13d3e6-498a-47bc-e729-d2f953cddfb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OshO48XLXQar"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6584, 48814)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_test),len(full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dj4tV8ytXTry"
   },
   "outputs": [],
   "source": [
    "# Приведем данные с списку списков, где слова будут кортежами\n",
    "fdata_train = []\n",
    "for sent in full_train:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наибольшая длина предложения 205\n",
      "Наибольшая длина токена 47\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
    "MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n",
    "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
    "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8772537323492737"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train)\n",
    "unigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=3165, backoff=89.74%, pruning=98.45%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8829828463586425"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger,verbose=True)\n",
    "bigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=4013, backoff=92.13%, pruning=98.76%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.882081353418933"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger,verbose=True)\n",
    "trigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=5953, backoff=91.77%, pruning=98.16%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.881769622215482"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_tagger = TrigramTagger(fdata_train, backoff=unigram_tagger,verbose=True)\n",
    "trigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=1183, backoff=90.22%, pruning=99.42%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8827385164964783"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_tagger = BigramTagger(fdata_train, backoff=trigram_tagger,verbose=True)\n",
    "bigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=561, backoff=83.58%, pruning=99.48%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8773801098641864"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train,backoff=bigram_tagger,verbose=True)\n",
    "unigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trained Unigram tagger: size=561, backoff=83.58%, pruning=99.48%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8773801098641864"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_tagger = UnigramTagger(fdata_train,backoff=trigram_tagger,verbose=True)\n",
    "unigram_tagger.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделим отдельно списки токенов и с списки меток в трейне и тесте\n",
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Анкета', '.', 'Начальник', 'областного', 'управления'],\n",
       " ['NOUN', 'PUNCT', 'NOUN', 'ADJ', 'NOUN'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tok[:5],train_label[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from lightgbm import LGBMClassifier \n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7, 13,  7, ...,  9,  7, 13], dtype=int64),\n",
       " array([ 7, 13,  1, ..., 10, 16, 13], dtype=int64),\n",
       " array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "        'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "        'VERB', 'X'], dtype='<U6'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_enc_labels,test_enc_labels,le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((871526, 98088), 871526)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countevctorizer = CountVectorizer(lowercase=True,analyzer='word',ngram_range=(1, 1),dtype = np.float32 )\n",
    "X_train = countevctorizer.fit_transform(train_tok)\n",
    "X_test = countevctorizer.transform(test_tok)\n",
    "X_train.shape,len(train_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.75674855929633"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr = LogisticRegression(random_state=0,max_iter=2000,n_jobs = 15)\n",
    "lr.fit(X_train, train_enc_labels)\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19120075489502242"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lgbc = LGBMClassifier()\n",
    "lgbc.fit(X_train, train_enc_labels)\n",
    "pred = lgbc.predict(X_test)\n",
    "accuracy_score(test_enc_labels, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectrasers = {'Count_word_lowercase_True_(1,1)': CountVectorizer(lowercase=True,analyzer='word',ngram_range=(1, 1),dtype = np.float32 ),\n",
    "              'Count_word_lowercase_True_(1,2)': CountVectorizer(lowercase=True,analyzer='word',ngram_range=(1, 2),dtype = np.float32 ),\n",
    "              'Count_word_lowercase_True_(1,3)': CountVectorizer(lowercase=True,analyzer='word',ngram_range=(1, 3),dtype = np.float32 ),\n",
    "              'Count_word_lowercase_False_(1,1)': CountVectorizer(lowercase=False,analyzer='word',ngram_range=(1, 1),dtype = np.float32 ),\n",
    "              'Count_word_lowercase_False_(1,2)': CountVectorizer(lowercase=False,analyzer='word',ngram_range=(1, 2),dtype = np.float32 ),\n",
    "              'Count_word_lowercase_False_(1,3)': CountVectorizer(lowercase=False,analyzer='word',ngram_range=(1, 3),dtype = np.float32 ),\n",
    "              'Count_char_wb_lowercase_False_(1,4)': CountVectorizer(lowercase=False,analyzer='char_wb',ngram_range=(1, 4),dtype = np.float32 ),\n",
    "              'Count_char_wb_lowercase_False_(1,6)': CountVectorizer(lowercase=False,analyzer='char_wb',ngram_range=(1, 6),dtype = np.float32 ),\n",
    "              'Count_char_wb_lowercase_False_(2,10)': CountVectorizer(lowercase=False,analyzer='char_wb',ngram_range=(2, 10),dtype = np.float32 ),\n",
    "              'Hash_char_(1,5)_f_500':HashingVectorizer(ngram_range=(1, 5), analyzer='char', n_features=1000),\n",
    "              'Hash_word_(1,1)_f_500':HashingVectorizer(ngram_range=(1, 1), analyzer='word', n_features=1000),\n",
    "              'Hash_word_(1,2)_f_500':HashingVectorizer(ngram_range=(1, 2), analyzer='word', n_features=1000),\n",
    "              'Hash_word_(1,3)_f_500':HashingVectorizer(ngram_range=(1, 3), analyzer='word', n_features=1000),\n",
    "              'Hash_char_(1,3)_f_500':HashingVectorizer(ngram_range=(1, 10), analyzer='char', n_features=1000),\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3h 29min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for vec in vectrasers:\n",
    "    X_train = vectrasers[vec].fit_transform(train_tok)\n",
    "    X_test = vectrasers[vec].transform(test_tok) \n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    l = accuracy_score(test_enc_labels, pred)\n",
    "    result = result.append({'vect':vec,'model':'lr','res':l},ignore_index=True)\n",
    "    \n",
    "    lgbc.fit(X_train, train_enc_labels)\n",
    "    \n",
    "    pred = lgbc.predict(X_test)\n",
    "    b = accuracy_score(test_enc_labels, pred)\n",
    "    \n",
    "    result = result.append({'vect':vec,'model':'lg','res':b},ignore_index=True)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vect</th>\n",
       "      <th>model</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,6)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.963300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Count_char_wb_lowercase_False_(2,10)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.963098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,4)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.962407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Hash_char_(1,5)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.888122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hash_char_(1,3)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.871702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hash_char_(1,3)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.811259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Count_word_lowercase_True_(1,3)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.757507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Count_word_lowercase_True_(1,2)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.757507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Count_word_lowercase_True_(1,1)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.756749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Count_word_lowercase_False_(1,3)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.750329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Count_word_lowercase_False_(1,2)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.750320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Count_word_lowercase_False_(1,1)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.749730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Hash_word_(1,2)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.578084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Hash_word_(1,1)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.577832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hash_char_(1,5)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.503437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hash_word_(1,3)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.376411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Hash_word_(1,2)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.376394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Hash_word_(1,1)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.375956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,4)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.367068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,6)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.232282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Count_char_wb_lowercase_False_(2,10)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.199415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Count_word_lowercase_True_(1,1)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.191201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Count_word_lowercase_False_(1,3)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.115897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Count_word_lowercase_False_(1,2)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.115897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hash_word_(1,3)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.089762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Count_word_lowercase_False_(1,1)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.049767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Count_word_lowercase_True_(1,3)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.018746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Count_word_lowercase_True_(1,2)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.018746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    vect model       res\n",
       "14   Count_char_wb_lowercase_False_(1,6)    lr  0.963300\n",
       "16  Count_char_wb_lowercase_False_(2,10)    lr  0.963098\n",
       "12   Count_char_wb_lowercase_False_(1,4)    lr  0.962407\n",
       "18                 Hash_char_(1,5)_f_500    lr  0.888122\n",
       "26                 Hash_char_(1,3)_f_500    lr  0.871702\n",
       "27                 Hash_char_(1,3)_f_500    lg  0.811259\n",
       "4        Count_word_lowercase_True_(1,3)    lr  0.757507\n",
       "2        Count_word_lowercase_True_(1,2)    lr  0.757507\n",
       "0        Count_word_lowercase_True_(1,1)    lr  0.756749\n",
       "10      Count_word_lowercase_False_(1,3)    lr  0.750329\n",
       "8       Count_word_lowercase_False_(1,2)    lr  0.750320\n",
       "6       Count_word_lowercase_False_(1,1)    lr  0.749730\n",
       "23                 Hash_word_(1,2)_f_500    lg  0.578084\n",
       "21                 Hash_word_(1,1)_f_500    lg  0.577832\n",
       "19                 Hash_char_(1,5)_f_500    lg  0.503437\n",
       "24                 Hash_word_(1,3)_f_500    lr  0.376411\n",
       "22                 Hash_word_(1,2)_f_500    lr  0.376394\n",
       "20                 Hash_word_(1,1)_f_500    lr  0.375956\n",
       "13   Count_char_wb_lowercase_False_(1,4)    lg  0.367068\n",
       "15   Count_char_wb_lowercase_False_(1,6)    lg  0.232282\n",
       "17  Count_char_wb_lowercase_False_(2,10)    lg  0.199415\n",
       "1        Count_word_lowercase_True_(1,1)    lg  0.191201\n",
       "11      Count_word_lowercase_False_(1,3)    lg  0.115897\n",
       "9       Count_word_lowercase_False_(1,2)    lg  0.115897\n",
       "25                 Hash_word_(1,3)_f_500    lg  0.089762\n",
       "7       Count_word_lowercase_False_(1,1)    lg  0.049767\n",
       "5        Count_word_lowercase_True_(1,3)    lg  0.018746\n",
       "3        Count_word_lowercase_True_(1,2)    lg  0.018746"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sort_values('res',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить прекрасные результаты дает логистическая регрессия на коунт векторах по буквенно (1,6),(2,10),(1,4). В пределах слова.  \n",
    "Бустинг вообще работает только с хэш ветором. В любом случае analyzer='word', хуже чем char\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cINqgGpKXURp"
   },
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCM0drjKXYet"
   },
   "source": [
    "много дополнительных датасетов на русском языке\n",
    "\n",
    "https://natasha.github.io/corus/  \n",
    "https://github.com/natasha/corus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUOg4C8sZNpw"
   },
   "source": [
    "мы будем использовать данные http://www.labinform.ru/pub/named_entities/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzi6ApNLZg6X"
   },
   "source": [
    "**Проверить насколько хорошо работает NER**\n",
    "\n",
    "1. взять нер из nltk\n",
    "2. проверить deeppavlov\n",
    "3. написать свой нер попробовать разные подходы:\n",
    "* передаём в сетку токен и его соседей\n",
    "* передаём в сетку только токен\n",
    "\n",
    "4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP1LgaNUtaOz"
   },
   "source": [
    "при обучении своего нера незабудьте разделить выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corus\n",
    "from corus import load_ne5\n",
    "from razdel import tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_coll_5 = r'D:\\train\\pos\\collection5/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UEdS2pAS3fod",
    "outputId": "402714b1-6931-41ce-ef39-f68080d9a29e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ne5Markup(\n",
       "    id='001',\n",
       "    text='Россия рассчитывает на конструктивное воздействие США на Грузию\\r\\n\\r\\n04/08/2008 12:08\\r\\n\\r\\nМОСКВА, 4 авг - РИА Новости. Россия рассчитывает, что США воздействуют на Тбилиси в связи с обострением ситуации в зоне грузино-осетинского конфликта. Об этом статс-секретарь - заместитель министра иностранных дел России Григорий Карасин заявил в телефонном разговоре с заместителем госсекретаря США Дэниэлом Фридом.\\r\\n\\r\\n\"С российской стороны выражена глубокая озабоченность в связи с новым витком напряженности вокруг Южной Осетии, противозаконными действиями грузинской стороны по наращиванию своих вооруженных сил в регионе, бесконтрольным строительством фортификационных сооружений\", - говорится в сообщении.\\r\\n\\r\\n\"Россия уже призвала Тбилиси к ответственной линии и рассчитывает также на конструктивное воздействие со стороны Вашингтона\", - сообщил МИД России. ',\n",
       "    spans=[Ne5Span(\n",
       "         index='T1',\n",
       "         type='GEOPOLIT',\n",
       "         start=0,\n",
       "         stop=6,\n",
       "         text='Россия'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T2',\n",
       "         type='GEOPOLIT',\n",
       "         start=50,\n",
       "         stop=53,\n",
       "         text='США'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T3',\n",
       "         type='GEOPOLIT',\n",
       "         start=57,\n",
       "         stop=63,\n",
       "         text='Грузию'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T4',\n",
       "         type='LOC',\n",
       "         start=87,\n",
       "         stop=93,\n",
       "         text='МОСКВА'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T5',\n",
       "         type='MEDIA',\n",
       "         start=103,\n",
       "         stop=114,\n",
       "         text='РИА Новости'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T6',\n",
       "         type='GEOPOLIT',\n",
       "         start=116,\n",
       "         stop=122,\n",
       "         text='Россия'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T7',\n",
       "         type='GEOPOLIT',\n",
       "         start=141,\n",
       "         stop=144,\n",
       "         text='США'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T8',\n",
       "         type='GEOPOLIT',\n",
       "         start=161,\n",
       "         stop=168,\n",
       "         text='Тбилиси'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T9',\n",
       "         type='GEOPOLIT',\n",
       "         start=301,\n",
       "         stop=307,\n",
       "         text='России'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T10',\n",
       "         type='PER',\n",
       "         start=308,\n",
       "         stop=324,\n",
       "         text='Григорий Карасин'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T11',\n",
       "         type='GEOPOLIT',\n",
       "         start=383,\n",
       "         stop=386,\n",
       "         text='США'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T12',\n",
       "         type='PER',\n",
       "         start=387,\n",
       "         stop=402,\n",
       "         text='Дэниэлом Фридом'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T13',\n",
       "         type='GEOPOLIT',\n",
       "         start=505,\n",
       "         stop=517,\n",
       "         text='Южной Осетии'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T14',\n",
       "         type='GEOPOLIT',\n",
       "         start=703,\n",
       "         stop=709,\n",
       "         text='Россия'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T15',\n",
       "         type='GEOPOLIT',\n",
       "         start=723,\n",
       "         stop=730,\n",
       "         text='Тбилиси'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T16',\n",
       "         type='GEOPOLIT',\n",
       "         start=815,\n",
       "         stop=825,\n",
       "         text='Вашингтона'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T17',\n",
       "         type='ORG',\n",
       "         start=838,\n",
       "         stop=841,\n",
       "         text='МИД'\n",
       "     ),\n",
       "     Ne5Span(\n",
       "         index='T18',\n",
       "         type='GEOPOLIT',\n",
       "         start=842,\n",
       "         stop=848,\n",
       "         text='России'\n",
       "     )]\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = load_ne5(patch_coll_5)\n",
    "next(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrhLNgNwQP2P"
   },
   "source": [
    "процедуры обработки взять из вебинарного ноутбука"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "uRuODJpkIqlv"
   },
   "outputs": [],
   "source": [
    "words_docs = []\n",
    "docs_words=[]\n",
    "for ix, rec in enumerate(records):\n",
    "    words = []\n",
    "    for token in tokenize(rec.text):\n",
    "        type_ent = 'OUT'\n",
    "        for ent in rec.spans:\n",
    "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
    "                type_ent = ent.type\n",
    "                break\n",
    "        words.append([token.text, type_ent])\n",
    "    docs_words.append([rec.text,words]) \n",
    "    words_docs.extend(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "XDdnL6EXJRt9"
   },
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])\n",
    "df_docs = pd.DataFrame(docs_words, columns=['text', 'tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skYaNCiC5xM4"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2kd-emBao1u-",
    "outputId": "a31344d7-177b-4b5f-b49e-4d4201f529ff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Комиссар СЕ критикует ограничительную политику...</td>\n",
       "      <td>[[Комиссар, OUT], [СЕ, ORG], [критикует, OUT],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Пулеметы, автоматы и снайперские винтовки изъя...</td>\n",
       "      <td>[[Пулеметы, OUT], [,, OUT], [автоматы, OUT], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4 октября назначены очередные выборы Верховног...</td>\n",
       "      <td>[[4, OUT], [октября, OUT], [назначены, OUT], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Следственное управление при прокуратуре требуе...</td>\n",
       "      <td>[[Следственное, ORG], [управление, ORG], [при,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>В Нижегородской области осудили бывшего началь...</td>\n",
       "      <td>[[В, OUT], [Нижегородской, LOC], [области, LOC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Депутат от \"ЕР\": К отставке А.Сердюкова причас...</td>\n",
       "      <td>[[Депутат, OUT], [от, OUT], [\", ORG], [ЕР, ORG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>\\r\\nСи Цзиньпин избран генсеком Коммунистическ...</td>\n",
       "      <td>[[Си, PER], [Цзиньпин, PER], [избран, OUT], [г...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>\"Ведомости\" узнали о смене лидера московских е...</td>\n",
       "      <td>[[\", MEDIA], [Ведомости, MEDIA], [\", MEDIA], [...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>СМИ узнали о кутежах туркменского чиновника на...</td>\n",
       "      <td>[[СМИ, MEDIA], [узнали, OUT], [о, OUT], [кутеж...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Вице-мэром Новосибирска по социальным вопросам...</td>\n",
       "      <td>[[Вице-мэром, OUT], [Новосибирска, LOC], [по, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    Комиссар СЕ критикует ограничительную политику...   \n",
       "1    Пулеметы, автоматы и снайперские винтовки изъя...   \n",
       "2    4 октября назначены очередные выборы Верховног...   \n",
       "3    Следственное управление при прокуратуре требуе...   \n",
       "4    В Нижегородской области осудили бывшего началь...   \n",
       "..                                                 ...   \n",
       "994  Депутат от \"ЕР\": К отставке А.Сердюкова причас...   \n",
       "995  \\r\\nСи Цзиньпин избран генсеком Коммунистическ...   \n",
       "996  \"Ведомости\" узнали о смене лидера московских е...   \n",
       "997  СМИ узнали о кутежах туркменского чиновника на...   \n",
       "998  Вице-мэром Новосибирска по социальным вопросам...   \n",
       "\n",
       "                                                   tag  \n",
       "0    [[Комиссар, OUT], [СЕ, ORG], [критикует, OUT],...  \n",
       "1    [[Пулеметы, OUT], [,, OUT], [автоматы, OUT], [...  \n",
       "2    [[4, OUT], [октября, OUT], [назначены, OUT], [...  \n",
       "3    [[Следственное, ORG], [управление, ORG], [при,...  \n",
       "4    [[В, OUT], [Нижегородской, LOC], [области, LOC...  \n",
       "..                                                 ...  \n",
       "994  [[Депутат, OUT], [от, OUT], [\", ORG], [ЕР, ORG...  \n",
       "995  [[Си, PER], [Цзиньпин, PER], [избран, OUT], [г...  \n",
       "996  [[\", MEDIA], [Ведомости, MEDIA], [\", MEDIA], [...  \n",
       "997  [[СМИ, MEDIA], [узнали, OUT], [о, OUT], [кутеж...  \n",
       "998  [[Вице-мэром, OUT], [Новосибирска, LOC], [по, ...  \n",
       "\n",
       "[999 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "KY96lqBzsZJ_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OUT         219112\n",
       "PER          21196\n",
       "ORG          13650\n",
       "LOC           4567\n",
       "GEOPOLIT      4342\n",
       "MEDIA         2480\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "6KE7tpVWs1b7"
   },
   "outputs": [],
   "source": [
    "def show_ner_text(n):\n",
    "    print(\"TEXT\")\n",
    "    print()\n",
    "    print(df_docs.text.loc[n])\n",
    "    print()\n",
    "    print('NER')  \n",
    "    print()\n",
    "    print([i for i in df_docs.tag.loc[n] if i[1]!='OUT'])\n",
    "    l = {i[1] for i in df_docs.tag.loc[n]}-{\"OUT\"}\n",
    "    print()\n",
    "    print('wich one ner')      \n",
    "    print(l)\n",
    "    print(len(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wsegbgCbrzy_",
    "outputId": "98fbfd4e-0fff-433a-9c24-2fa56ab7d241",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT\n",
      "\n",
      "Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\r\n",
      "\r\n",
      "05/08/2008 10:32\r\n",
      "\r\n",
      "МОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.\r\n",
      "\r\n",
      "\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.\r\n",
      "\r\n",
      "По сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".\r\n",
      "\r\n",
      "Комиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".\r\n",
      "\r\n",
      "\"Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами\", - замечает он.\r\n",
      "\r\n",
      "Комиссар Совета Европы призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках ООН и ЕС.\n",
      "\n",
      "NER\n",
      "\n",
      "[['СЕ', 'ORG'], ['МОСКВА', 'GEOPOLIT'], ['Новости-Грузия', 'MEDIA'], ['Совета', 'ORG'], ['Европы', 'ORG'], ['Томас', 'PER'], ['Хаммарберг', 'PER'], ['(', 'PER'], ['Thomas', 'PER'], ['Hammarberg', 'PER'], [')', 'PER'], ['РИА', 'MEDIA'], ['Новости', 'MEDIA'], ['Хаммарберг', 'PER'], ['Совета', 'ORG'], ['Европы', 'ORG'], ['ООН', 'ORG'], ['ЕС', 'GEOPOLIT']]\n",
      "\n",
      "wich one ner\n",
      "{'ORG', 'MEDIA', 'PER', 'GEOPOLIT'}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "show_ner_text(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQ1phPKisJMz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_nltk(n):\n",
    "    print(\"TEXT\")\n",
    "    text = df_docs.text.loc[n]\n",
    "    print()\n",
    "    print(text)\n",
    "    print()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    ner = {(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(pos) if hasattr(chunk, 'label') }\n",
    "    print('NER')  \n",
    "    print()\n",
    "    print(ner)\n",
    "    l = {i[1] for i in df_docs.tag.loc[n]}-{\"OUT\"}\n",
    "    print()\n",
    "#     print('wich one ner')      \n",
    "#     print(l)\n",
    "#     print(len(l))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT\n",
      "\n",
      "Комиссар СЕ критикует ограничительную политику в отношении беженцев в европейских странах\n",
      "\n",
      "05/08/2008 10:32\n",
      "\n",
      "МОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар Совета Европы по правам человека Томас Хаммарберг (Thomas Hammarberg) в размещенном на его сайте еженедельном комментарии.\n",
      "\n",
      "\"Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей\", - полагает он.\n",
      "\n",
      "По сообщению РИА Новости, Хаммарберг констатирует, что в последнее время \"правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране\".\n",
      "\n",
      "Комиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела \"к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными\".\n",
      "\n",
      "\"Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами\", - замечает он.\n",
      "\n",
      "Комиссар Совета Европы призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках ООН и ЕС.\n",
      "\n",
      "NER\n",
      "\n",
      "{('Комиссар', 'PERSON'), ('Хаммарберг', 'PERSON'), ('Совета Европы', 'PERSON'), ('МОСКВА', 'ORGANIZATION'), ('СЕ', 'ORGANIZATION'), ('Thomas Hammarberg', 'PERSON'), ('РИА Новости', 'ORGANIZATION')}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ner_nltk(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "train_x, test_x, train_y , test_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n",
    "\n",
    "# labelEncode целевую переменную\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 3, ..., 4, 4, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GEOPOLIT', 'LOC', 'MEDIA', 'ORG', 'OUT', 'PER'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188198         штате\n",
       " 102970         уходу\n",
       " 219144             e\n",
       " 95130      освободил\n",
       " 259378           Она\n",
       "              ...    \n",
       " 222468         сумму\n",
       " 89881          своем\n",
       " 192373    президенты\n",
       " 149982             и\n",
       " 117937        активы\n",
       " Name: word, Length: 199010, dtype: object,\n",
       " 201232              -\n",
       " 88375               ,\n",
       " 255007             их\n",
       " 179781         Джобса\n",
       " 9760      экстремизма\n",
       "              ...     \n",
       " 63391               в\n",
       " 135935       Напомним\n",
       " 209141              .\n",
       " 176269         личных\n",
       " 13846       принимала\n",
       " Name: word, Length: 66337, dtype: object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x,test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ner = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              vect model       res\n",
      "0  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "                              vect model       res\n",
      "0  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2  Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "                              vect model       res\n",
      "0  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2  Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3  Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "                              vect model       res\n",
      "0  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2  Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3  Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4  Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "                              vect model       res\n",
      "0  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2  Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3  Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4  Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5  Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "                              vect model       res\n",
      "0  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1  Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2  Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3  Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4  Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5  Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6  Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "                               vect model       res\n",
      "0   Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1   Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2   Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3   Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4   Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5   Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6   Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7  Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "                               vect model       res\n",
      "0   Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1   Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2   Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3   Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4   Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5   Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6   Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7  Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8  Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "                               vect model       res\n",
      "0   Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1   Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2   Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3   Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4   Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5   Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6   Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7  Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8  Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9  Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "                                vect model       res\n",
      "0    Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1    Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2    Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3    Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4    Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5    Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6    Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7   Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8   Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9   Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10  Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "                                vect model       res\n",
      "0    Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1    Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2    Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3    Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4    Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5    Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6    Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7   Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8   Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9   Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10  Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11  Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "                                vect model       res\n",
      "0    Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1    Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2    Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3    Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4    Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5    Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6    Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7   Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8   Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9   Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10  Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11  Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12  Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "                                   vect model       res\n",
      "0       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2       Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3       Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4       Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5       Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6       Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7      Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8      Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9      Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10     Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11     Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12     Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13  Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "                                   vect model       res\n",
      "0       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2       Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3       Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4       Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5       Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6       Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7      Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8      Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9      Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10     Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11     Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12     Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13  Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14  Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "                                   vect model       res\n",
      "0       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2       Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3       Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4       Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5       Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6       Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7      Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8      Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9      Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10     Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11     Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12     Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13  Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14  Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15  Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "                                   vect model       res\n",
      "0       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1       Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2       Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3       Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4       Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5       Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6       Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7      Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8      Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9      Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10     Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11     Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12     Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13  Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14  Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15  Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16  Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "23                 Hash_word_(1,2)_f_500    lr  0.846873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "23                 Hash_word_(1,2)_f_500    lr  0.846873\n",
      "24                 Hash_word_(1,2)_f_500    lg  0.847129\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "23                 Hash_word_(1,2)_f_500    lr  0.846873\n",
      "24                 Hash_word_(1,2)_f_500    lg  0.847129\n",
      "25                 Hash_word_(1,3)_f_500    lr  0.846873\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "23                 Hash_word_(1,2)_f_500    lr  0.846873\n",
      "24                 Hash_word_(1,2)_f_500    lg  0.847129\n",
      "25                 Hash_word_(1,3)_f_500    lr  0.846873\n",
      "26                 Hash_word_(1,3)_f_500    lg  0.847114\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "23                 Hash_word_(1,2)_f_500    lr  0.846873\n",
      "24                 Hash_word_(1,2)_f_500    lg  0.847129\n",
      "25                 Hash_word_(1,3)_f_500    lr  0.846873\n",
      "26                 Hash_word_(1,3)_f_500    lg  0.847114\n",
      "27                 Hash_char_(1,3)_f_500    lr  0.895850\n",
      "                                    vect model       res\n",
      "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
      "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
      "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
      "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
      "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
      "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
      "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
      "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
      "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
      "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
      "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
      "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
      "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
      "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
      "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
      "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
      "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
      "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
      "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
      "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
      "21                 Hash_word_(1,1)_f_500    lr  0.846722\n",
      "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
      "23                 Hash_word_(1,2)_f_500    lr  0.846873\n",
      "24                 Hash_word_(1,2)_f_500    lg  0.847129\n",
      "25                 Hash_word_(1,3)_f_500    lr  0.846873\n",
      "26                 Hash_word_(1,3)_f_500    lg  0.847114\n",
      "27                 Hash_char_(1,3)_f_500    lr  0.895850\n",
      "28                 Hash_char_(1,3)_f_500    lg  0.930145\n",
      "Wall time: 12min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for vec in vectrasers:\n",
    "    X_train = vectrasers[vec].fit_transform(train_x)\n",
    "    X_test = vectrasers[vec].transform(test_x) \n",
    "    lr.fit(X_train, train_y)\n",
    "    pred = lr.predict(X_test)\n",
    "\n",
    "    l = accuracy_score(test_y, pred)\n",
    "    result_ner = result_ner.append({'vect':vec,'model':'lr','res':l},ignore_index=True)\n",
    "    print(result_ner)\n",
    "    \n",
    "    lgbc.fit(X_train, train_y)\n",
    "    \n",
    "    pred = lgbc.predict(X_test)\n",
    "    b = accuracy_score(test_y, pred)\n",
    "    \n",
    "    result_ner = result_ner.append({'vect':vec,'model':'lg','res':b},ignore_index=True)\n",
    "    print(result_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vect</th>\n",
       "      <th>model</th>\n",
       "      <th>res</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Count_char_wb_lowercase_False_(2,10)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.956947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,6)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.956872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,4)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.956193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,6)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.949048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Count_char_wb_lowercase_False_(1,4)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.948777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Count_char_wb_lowercase_False_(2,10)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.947269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Hash_char_(1,5)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.931139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Hash_char_(1,3)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.930145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Count_word_lowercase_False_(1,2)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.914316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Count_word_lowercase_False_(1,3)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.914271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Count_word_lowercase_False_(1,1)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.914166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Count_word_lowercase_True_(1,2)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.910548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Count_word_lowercase_True_(1,3)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.910548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Count_word_lowercase_True_(1,1)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.910337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Count_word_lowercase_True_(1,1)</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.910337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hash_char_(1,5)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.898187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Hash_char_(1,3)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.895850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Count_word_lowercase_False_(1,3)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.873992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Count_word_lowercase_False_(1,2)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.873992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Count_word_lowercase_False_(1,1)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.873992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Count_word_lowercase_True_(1,3)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.872801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Count_word_lowercase_True_(1,2)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.872801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Count_word_lowercase_True_(1,1)</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.872801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hash_word_(1,2)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.847129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Hash_word_(1,3)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.847114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Hash_word_(1,1)_f_500</td>\n",
       "      <td>lg</td>\n",
       "      <td>0.847054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Hash_word_(1,2)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.846873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hash_word_(1,3)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.846873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Hash_word_(1,1)_f_500</td>\n",
       "      <td>lr</td>\n",
       "      <td>0.846722</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    vect model       res\n",
       "17  Count_char_wb_lowercase_False_(2,10)    lr  0.956947\n",
       "15   Count_char_wb_lowercase_False_(1,6)    lr  0.956872\n",
       "13   Count_char_wb_lowercase_False_(1,4)    lr  0.956193\n",
       "16   Count_char_wb_lowercase_False_(1,6)    lg  0.949048\n",
       "14   Count_char_wb_lowercase_False_(1,4)    lg  0.948777\n",
       "18  Count_char_wb_lowercase_False_(2,10)    lg  0.947269\n",
       "20                 Hash_char_(1,5)_f_500    lg  0.931139\n",
       "28                 Hash_char_(1,3)_f_500    lg  0.930145\n",
       "9       Count_word_lowercase_False_(1,2)    lr  0.914316\n",
       "11      Count_word_lowercase_False_(1,3)    lr  0.914271\n",
       "7       Count_word_lowercase_False_(1,1)    lr  0.914166\n",
       "3        Count_word_lowercase_True_(1,2)    lr  0.910548\n",
       "5        Count_word_lowercase_True_(1,3)    lr  0.910548\n",
       "1        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
       "0        Count_word_lowercase_True_(1,1)    lr  0.910337\n",
       "19                 Hash_char_(1,5)_f_500    lr  0.898187\n",
       "27                 Hash_char_(1,3)_f_500    lr  0.895850\n",
       "12      Count_word_lowercase_False_(1,3)    lg  0.873992\n",
       "10      Count_word_lowercase_False_(1,2)    lg  0.873992\n",
       "8       Count_word_lowercase_False_(1,1)    lg  0.873992\n",
       "6        Count_word_lowercase_True_(1,3)    lg  0.872801\n",
       "4        Count_word_lowercase_True_(1,2)    lg  0.872801\n",
       "2        Count_word_lowercase_True_(1,1)    lg  0.872801\n",
       "24                 Hash_word_(1,2)_f_500    lg  0.847129\n",
       "26                 Hash_word_(1,3)_f_500    lg  0.847114\n",
       "22                 Hash_word_(1,1)_f_500    lg  0.847054\n",
       "23                 Hash_word_(1,2)_f_500    lr  0.846873\n",
       "25                 Hash_word_(1,3)_f_500    lr  0.846873\n",
       "21                 Hash_word_(1,1)_f_500    lr  0.846722"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_ner.sort_values('res',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, \\\n",
    "GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input,BatchNormalization\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.activations import relu,sigmoid,tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Комиссар</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>СЕ</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>критикует</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ограничительную</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>политику</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265342</th>\n",
       "      <td>замглавы</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265343</th>\n",
       "      <td>Бердска</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265344</th>\n",
       "      <td>Владимир</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265345</th>\n",
       "      <td>Штоп</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265346</th>\n",
       "      <td>.</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265347 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   word  tag\n",
       "0              Комиссар  OUT\n",
       "1                    СЕ  ORG\n",
       "2             критикует  OUT\n",
       "3       ограничительную  OUT\n",
       "4              политику  OUT\n",
       "...                 ...  ...\n",
       "265342         замглавы  OUT\n",
       "265343          Бердска  LOC\n",
       "265344         Владимир  PER\n",
       "265345             Штоп  PER\n",
       "265346                .  OUT\n",
       "\n",
       "[265347 rows x 2 columns]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sparse_matrix_to_sparse_tensor(X):\n",
    "    coo = X.tocoo()\n",
    "    indices = np.mat([coo.row, coo.col]).transpose()\n",
    "    return tf.SparseTensor(indices, coo.data, coo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y , test_y = model_selection.train_test_split(df_words['word'], df_words['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(lowercase=False,analyzer='word',ngram_range=(1, 1),dtype = np.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vec.fit_transform(train_x)\n",
    "X_test = vec.transform(test_x)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.transform(test_y)\n",
    "train_y = tf.one_hot(train_y,depth=6)\n",
    "test_y = tf.one_hot(test_y,depth=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((66337, 29260), (199010, 29260))"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape,X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = convert_sparse_matrix_to_sparse_tensor(X_train)\n",
    "X_train_ = tf.sparse.reorder(X_train_)\n",
    "X_test_ = convert_sparse_matrix_to_sparse_tensor(X_test)\n",
    "X_test_ = tf.sparse.reorder(X_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input = Input(shape=(29260))\n",
    "\n",
    "x = Dense(64)(x_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = sigmoid(x)\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tanh(x)\n",
    "x = Dense(16)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "output = Dense(6,activation='softmax')(x)\n",
    "\n",
    "model =Model(x_input,output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lqw\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_22/dense_85/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_22/dense_85/embedding_lookup_sparse/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_22/dense_85/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 2s 14ms/step - loss: 0.8367 - accuracy: 0.8262 - val_loss: 1.1010 - val_accuracy: 0.8282\n",
      "Epoch 2/10\n",
      "98/98 [==============================] - 1s 13ms/step - loss: 0.3087 - accuracy: 0.9516 - val_loss: 1.0424 - val_accuracy: 0.8282\n",
      "Epoch 3/10\n",
      "98/98 [==============================] - 1s 13ms/step - loss: 0.2114 - accuracy: 0.9578 - val_loss: 1.0058 - val_accuracy: 0.8778\n",
      "Epoch 4/10\n",
      "98/98 [==============================] - 1s 13ms/step - loss: 0.1778 - accuracy: 0.9599 - val_loss: 0.7232 - val_accuracy: 0.9276\n",
      "Epoch 5/10\n",
      "98/98 [==============================] - 1s 13ms/step - loss: 0.1622 - accuracy: 0.9605 - val_loss: 0.3881 - val_accuracy: 0.9369\n",
      "Epoch 6/10\n",
      "98/98 [==============================] - 1s 13ms/step - loss: 0.1526 - accuracy: 0.9610 - val_loss: 0.2790 - val_accuracy: 0.9386\n",
      "Epoch 7/10\n",
      "98/98 [==============================] - 1s 13ms/step - loss: 0.1476 - accuracy: 0.9611 - val_loss: 0.2444 - val_accuracy: 0.9392\n",
      "Epoch 8/10\n",
      "98/98 [==============================] - 1s 13ms/step - loss: 0.1441 - accuracy: 0.9614 - val_loss: 0.2378 - val_accuracy: 0.9393\n",
      "Epoch 9/10\n",
      "98/98 [==============================] - 1s 13ms/step - loss: 0.1417 - accuracy: 0.9615 - val_loss: 0.2379 - val_accuracy: 0.9393\n",
      "Epoch 10/10\n",
      "98/98 [==============================] - 1s 13ms/step - loss: 0.1398 - accuracy: 0.9614 - val_loss: 0.2396 - val_accuracy: 0.9391\n",
      "2074/2074 [==============================] - 5s 3ms/step - loss: 0.2396 - accuracy: 0.9391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23959730565547943, 0.9391440749168396]"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hh = model.fit(X_train_,train_y,epochs=10,batch_size = 2048,validation_data=(X_test_, test_y),verbose=1)\n",
    "model.evaluate(X_test_, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Комиссар</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>СЕ</td>\n",
       "      <td>ORG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>критикует</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ограничительную</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>политику</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265342</th>\n",
       "      <td>замглавы</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265343</th>\n",
       "      <td>Бердска</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265344</th>\n",
       "      <td>Владимир</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265345</th>\n",
       "      <td>Штоп</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265346</th>\n",
       "      <td>.</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265347 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   word  tag\n",
       "0              Комиссар  OUT\n",
       "1                    СЕ  ORG\n",
       "2             критикует  OUT\n",
       "3       ограничительную  OUT\n",
       "4              политику  OUT\n",
       "...                 ...  ...\n",
       "265342         замглавы  OUT\n",
       "265343          Бердска  LOC\n",
       "265344         Владимир  PER\n",
       "265345             Штоп  PER\n",
       "265346                .  OUT\n",
       "\n",
       "[265347 rows x 2 columns]"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим слова до и после ключевого"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "key_word = '.'\n",
    "befor_word = \".\"\n",
    "after_word = '.'\n",
    "\n",
    "for ind,i in df_words.word.items():\n",
    "    if i =='.':\n",
    "        words.append('.')\n",
    "        continue\n",
    "    key_word = i\n",
    "    if ind>1 :\n",
    "        befor_word = df_words.word.loc[ind-1]\n",
    "    else:\n",
    "        befor_word = \".\" \n",
    "    if ind<len(df_words) :\n",
    "        after_word = df_words.word.loc[ind+1]\n",
    "    else:\n",
    "        after_word = \".\"    \n",
    "        \n",
    "    words.append(', '.join([befor_word,key_word,after_word]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tag</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Комиссар</td>\n",
       "      <td>OUT</td>\n",
       "      <td>., Комиссар, СЕ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>СЕ</td>\n",
       "      <td>ORG</td>\n",
       "      <td>., СЕ, критикует</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>критикует</td>\n",
       "      <td>OUT</td>\n",
       "      <td>СЕ, критикует, ограничительную</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ограничительную</td>\n",
       "      <td>OUT</td>\n",
       "      <td>критикует, ограничительную, политику</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>политику</td>\n",
       "      <td>OUT</td>\n",
       "      <td>ограничительную, политику, в</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265342</th>\n",
       "      <td>замглавы</td>\n",
       "      <td>OUT</td>\n",
       "      <td>первый, замглавы, Бердска</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265343</th>\n",
       "      <td>Бердска</td>\n",
       "      <td>LOC</td>\n",
       "      <td>замглавы, Бердска, Владимир</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265344</th>\n",
       "      <td>Владимир</td>\n",
       "      <td>PER</td>\n",
       "      <td>Бердска, Владимир, Штоп</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265345</th>\n",
       "      <td>Штоп</td>\n",
       "      <td>PER</td>\n",
       "      <td>Владимир, Штоп, .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265346</th>\n",
       "      <td>.</td>\n",
       "      <td>OUT</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>265347 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   word  tag                                 words\n",
       "0              Комиссар  OUT                       ., Комиссар, СЕ\n",
       "1                    СЕ  ORG                      ., СЕ, критикует\n",
       "2             критикует  OUT        СЕ, критикует, ограничительную\n",
       "3       ограничительную  OUT  критикует, ограничительную, политику\n",
       "4              политику  OUT          ограничительную, политику, в\n",
       "...                 ...  ...                                   ...\n",
       "265342         замглавы  OUT             первый, замглавы, Бердска\n",
       "265343          Бердска  LOC           замглавы, Бердска, Владимир\n",
       "265344         Владимир  PER               Бердска, Владимир, Штоп\n",
       "265345             Штоп  PER                     Владимир, Штоп, .\n",
       "265346                .  OUT                                     .\n",
       "\n",
       "[265347 rows x 3 columns]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words['words'] = words\n",
    "df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([66337, 33705]), TensorShape([199010, 33705]))"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, test_x, train_y , test_y = model_selection.train_test_split(df_words['words'], df_words['tag'])\n",
    "vec = CountVectorizer(lowercase=False,analyzer='word',ngram_range=(1, 1),dtype = np.float32 )\n",
    "X_train = vec.fit_transform(train_x)\n",
    "X_test = vec.transform(test_x)\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.transform(test_y)\n",
    "train_y = tf.one_hot(train_y,depth=6)\n",
    "test_y = tf.one_hot(test_y,depth=6)\n",
    "X_train_ = convert_sparse_matrix_to_sparse_tensor(X_train)\n",
    "X_train_ = tf.sparse.reorder(X_train_)\n",
    "X_test_ = convert_sparse_matrix_to_sparse_tensor(X_test)\n",
    "X_test_ = tf.sparse.reorder(X_test_)\n",
    "X_test_.shape,X_train_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lqw\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:447: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_26/dense_101/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_26/dense_101/embedding_lookup_sparse/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_26/dense_101/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98/98 [==============================] - 2s 15ms/step - loss: 0.7730 - accuracy: 0.8265 - val_loss: 1.0085 - val_accuracy: 0.8246\n",
      "Epoch 2/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.3311 - accuracy: 0.9158 - val_loss: 0.7785 - val_accuracy: 0.8246\n",
      "Epoch 3/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.2220 - accuracy: 0.9328 - val_loss: 0.6021 - val_accuracy: 0.8255\n",
      "Epoch 4/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.1756 - accuracy: 0.9434 - val_loss: 0.4201 - val_accuracy: 0.8806\n",
      "Epoch 5/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.1480 - accuracy: 0.9512 - val_loss: 0.3297 - val_accuracy: 0.8881\n",
      "Epoch 6/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.1290 - accuracy: 0.9569 - val_loss: 0.2941 - val_accuracy: 0.8995\n",
      "Epoch 7/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.1151 - accuracy: 0.9613 - val_loss: 0.3025 - val_accuracy: 0.8993\n",
      "Epoch 8/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.1049 - accuracy: 0.9643 - val_loss: 0.3093 - val_accuracy: 0.8964\n",
      "Epoch 9/20\n",
      "98/98 [==============================] - 2s 13ms/step - loss: 0.0972 - accuracy: 0.9664 - val_loss: 0.3205 - val_accuracy: 0.8973\n",
      "Epoch 10/20\n",
      "98/98 [==============================] - 2s 13ms/step - loss: 0.0915 - accuracy: 0.9680 - val_loss: 0.3347 - val_accuracy: 0.8930\n",
      "Epoch 11/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.0867 - accuracy: 0.9690 - val_loss: 0.3386 - val_accuracy: 0.8957\n",
      "Epoch 12/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.0832 - accuracy: 0.9698 - val_loss: 0.3502 - val_accuracy: 0.8941\n",
      "Epoch 13/20\n",
      "98/98 [==============================] - 2s 13ms/step - loss: 0.0803 - accuracy: 0.9705 - val_loss: 0.3557 - val_accuracy: 0.8933\n",
      "Epoch 14/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.0782 - accuracy: 0.9709 - val_loss: 0.3633 - val_accuracy: 0.8945\n",
      "Epoch 15/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.0763 - accuracy: 0.9712 - val_loss: 0.3698 - val_accuracy: 0.8939\n",
      "Epoch 16/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.0752 - accuracy: 0.9714 - val_loss: 0.3741 - val_accuracy: 0.8941\n",
      "Epoch 17/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.0738 - accuracy: 0.9715 - val_loss: 0.3821 - val_accuracy: 0.8934\n",
      "Epoch 18/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.0727 - accuracy: 0.9717 - val_loss: 0.3830 - val_accuracy: 0.8940\n",
      "Epoch 19/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.0725 - accuracy: 0.9716 - val_loss: 0.3882 - val_accuracy: 0.8936\n",
      "Epoch 20/20\n",
      "98/98 [==============================] - 2s 14ms/step - loss: 0.0712 - accuracy: 0.9717 - val_loss: 0.3946 - val_accuracy: 0.8952\n",
      "2074/2074 [==============================] - 5s 3ms/step - loss: 0.3946 - accuracy: 0.8952\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3945581018924713, 0.8952168226242065]"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input = Input(shape=(33705))\n",
    "\n",
    "x = Dense(64)(x_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = sigmoid(x)\n",
    "x = Dense(32)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = tanh(x)\n",
    "x = Dense(16)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = relu(x)\n",
    "output = Dense(6,activation='softmax')(x)\n",
    "\n",
    "model =Model(x_input,output)\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hh_1 = model.fit(X_train_,train_y,epochs=20,batch_size = 2048,validation_data=(X_test_, test_y),verbose=1)\n",
    "model.evaluate(X_test_, test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом простая сеть хуже решает задачу. Добовление смежных слов , ухудштло показатели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW5-colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
